# ğŸ“‹ AnÃ¡lise de Conformidade - Tech Challenge 2 (MLET)

## Resumo Executivo

âœ… **7/8 requisitos atendidos**  
âš ï¸ 1 requisito corrigido durante a anÃ¡lise

---

## AnÃ¡lise Detalhada por Requisito

### âœ… Requisito 1: Scrap de dados B3 (granularidade diÃ¡ria)
**Status: ATENDE**

- **Arquivo:** `functions/extract.py`
- **MÃ©todo:** Biblioteca `yfinance`
- **Tickers:** ITUB4.SA, BBDC4.SA, BBAS3.SA (Blue Chips - Bancos)
- **Granularidade:** Dados diÃ¡rios
- **PerÃ­odo:** Ãšltimos 6 meses (configurable)

**EvidÃªncias:**
- Lines 22-27: DefiniÃ§Ã£o dos tickers
- Lines 34-69: FunÃ§Ã£o `download_ticker_data()` usando yfinance
- Lines 191-193: ConfiguraÃ§Ã£o do perÃ­odo (D-1 atÃ© 6 meses antes)

---

### âœ… Requisito 2: Dados brutos no S3 em parquet com partiÃ§Ã£o diÃ¡ria
**Status: ATENDE**

- **Arquivo:** `functions/extract.py`
- **Formato:** Parquet (PyArrow)
- **Particionamento:** Por `data_particao` (data da cotaÃ§Ã£o)
- **Path:** `s3://bucket/raw/execution_date=YYYY-MM-DD/run_timestamp/data_particao=YYYY-MM-DD/*.parquet`

**EvidÃªncias:**
- Lines 119-156: FunÃ§Ã£o `save_to_parquet_partitioned()`
- Line 141: CriaÃ§Ã£o da coluna `data_particao` 
- Lines 147-151: `pq.write_to_dataset()` com `partition_cols=['data_particao']`

**Estrutura do Particionamento:**
```
raw/
â””â”€â”€ execution_date=2026-01-14/
    â””â”€â”€ run_20260114_220000/
        â”œâ”€â”€ data_particao=2024-07-15/
        â”‚   â””â”€â”€ part-0.parquet
        â”œâ”€â”€ data_particao=2024-07-16/
        â”‚   â””â”€â”€ part-0.parquet
        â””â”€â”€ _SUCCESS
```

---

### âœ… Requisito 3: Bucket aciona Lambda que chama job Glue
**Status: ATENDE**

- **Arquivo:** `terraform/lambda.tf`
- **Trigger:** S3 Bucket Notification
- **Evento:** `s3:ObjectCreated:*`
- **Filtros:** 
  - Prefixo: `raw/`
  - Sufixo: `_SUCCESS`

**EvidÃªncias:**
- Lines 121-128 (lambda.tf): ConfiguraÃ§Ã£o do `aws_s3_bucket_notification`
- Lines 116-120 (lambda.tf): PermissÃ£o para S3 invocar Lambda
- `functions/trigger_glue.py`: Lambda que recebe evento S3 e inicia job Glue

**Fluxo:**
1. Extract Lambda salva dados em `raw/.../*.parquet`
2. Extract Lambda cria arquivo `_SUCCESS` como marker
3. S3 notification aciona `s3_trigger_glue` Lambda
4. Lambda `trigger_glue.py` inicia job Glue

---

### âœ… Requisito 4: Lambda em qualquer linguagem
**Status: ATENDE**

- **Arquivo:** `functions/trigger_glue.py`
- **Linguagem:** Python 3.10
- **FunÃ§Ã£o:** Apenas iniciar job Glue (conforme especificado)

**EvidÃªncias:**
- Lines 62-68 (trigger_glue.py): `glue.start_job_run()` com argumentos
- ProteÃ§Ã£o contra execuÃ§Ãµes concorrentes (lines 11-19)
- Tratamento de eventos S3 e invocaÃ§Ãµes manuais (lines 21-47)

---

### âœ… Requisito 5: Job Glue com transformaÃ§Ãµes obrigatÃ³rias
**Status: ATENDE TODOS OS SUB-REQUISITOS**

#### 5.A - Agrupamento/SumarizaÃ§Ã£o âœ…
- **Arquivo:** `src/transform.py` (lines 193-215)
- **Agrupamento:** Por `nome_acao` e `mes_referencia`
- **AgregaÃ§Ãµes:**
  - PreÃ§os: mÃ©dia, mÃ­nimo, mÃ¡ximo mensal
  - Volume: total mensal, mÃ©dia diÃ¡ria
  - EstatÃ­sticas: variaÃ§Ã£o mÃ©dia diÃ¡ria, volatilidade mÃ©dia mensal
  - Contagem: dias de negociaÃ§Ã£o no mÃªs

```python
df_agregado = df_refined.group_by([
    "nome_acao",
    pl.col("data_pregao").dt.truncate("1mo").alias("mes_referencia")
]).agg([
    pl.col("fechamento").mean().alias("preco_medio_mensal"),
    pl.col("volume_negociado").sum().alias("volume_total_mensal"),
    ...
])
```

#### 5.B - Renomear colunas âœ…
- **Arquivo:** `src/transform.py` (lines 107-113)
- **RenomeaÃ§Ãµes realizadas:**
  - `Date` â†’ `data_pregao`
  - `Ticker` â†’ `nome_acao`
  - `Open` â†’ `abertura`
  - `Close` â†’ `fechamento`
  - `High` â†’ `max`
  - `Low` â†’ `min`
  - `Volume` â†’ `volume_negociado`

**Total:** 7 colunas renomeadas (requisito pede 2 + colunas de agrupamento)

#### 5.C - CÃ¡lculo com data âœ…
- **Arquivo:** `src/transform.py` (lines 115-126)
- **CÃ¡lculos temporais implementados:**

1. **MÃ©dias MÃ³veis:**
   - 7 dias: `media_movel_7d`
   - 14 dias: `media_movel_14d`
   - 30 dias: `media_movel_30d`

2. **Lags (valores anteriores):**
   - 1 dia: `lag_1d`
   - 2 dias: `lag_2d`
   - 3 dias: `lag_3d`

3. **Volatilidade:**
   - Desvio padrÃ£o mÃ³vel de 7 dias: `volatilidade_7d`

4. **Outras:**
   - VariaÃ§Ã£o percentual diÃ¡ria: `variacao_pct_dia`
   - Amplitude do dia: `amplitude_dia`

```python
pl.col("Close").rolling_mean(window_size=7).over("Ticker").alias("media_movel_7d"),
pl.col("Close").shift(1).over("Ticker").alias("lag_1d"),
pl.col("Close").rolling_std(window_size=7).over("Ticker").alias("volatilidade_7d"),
```

---

### âœ… Requisito 6: Dados refined particionados por data e aÃ§Ã£o
**Status: ATENDE**

- **Arquivo:** `src/transform.py` (lines 166-177)
- **Path:** `s3://bucket/refined/`
- **Formato:** Parquet (Snappy compression)
- **Particionamento:** `partition_by=["data_pregao", "nome_acao"]`

**EvidÃªncias:**
```python
df_final.write_parquet(
    output_path_refined,
    use_pyarrow=True,
    partition_by=["data_pregao", "nome_acao"],
    compression="snappy"
)
```

**Estrutura resultante:**
```
refined/
â”œâ”€â”€ data_pregao=2024-07-15/
â”‚   â”œâ”€â”€ nome_acao=itub4/
â”‚   â”‚   â””â”€â”€ data.parquet
â”‚   â”œâ”€â”€ nome_acao=bbdc4/
â”‚   â”‚   â””â”€â”€ data.parquet
â”‚   â””â”€â”€ nome_acao=bbas3/
â”‚       â””â”€â”€ data.parquet
â””â”€â”€ data_pregao=2024-07-16/
    â””â”€â”€ ...
```

---

### âœ… Requisito 7: Catalogar automaticamente no Glue Catalog
**Status: ATENDE (APÃ“S CORREÃ‡ÃƒO)**

- **Arquivo:** `src/transform.py` (seÃ§Ã£o 5 - recÃ©m adicionada)
- **Database:** `default`
- **Tabelas criadas:**
  1. `refined_stocks` (com partiÃ§Ãµes por data_pregao e nome_acao)
  2. `aggregated_stocks_monthly`

**ImplementaÃ§Ã£o:**
- Usa boto3 para criar/atualizar tabelas no Glue Catalog
- Schema completo definido para ambas as tabelas
- ConfiguraÃ§Ã£o de SerDe para Parquet
- PartiÃ§Ãµes configuradas para tabela refined
- Try/except para criar ou atualizar tabelas existentes

**EvidÃªncias:**
- Lines 234-373: Bloco completo de catalogaÃ§Ã£o automÃ¡tica
- Schemas detalhados para ambas as tabelas
- Tratamento de exceÃ§Ã£o `AlreadyExistsException` para atualizar tabelas

**Recursos adicionais:**
- `terraform/glue.tf` (lines 75-92): Crawler configurado como backup
- Crawler roda diariamente Ã s 23:00 UTC (apÃ³s ETL Ã s 22:00 UTC)

---

### âœ… Requisito 8: Dados consultÃ¡veis via Athena
**Status: ATENDE**

- **Arquivo:** `terraform/athena.tf`
- **Workgroup:** `etl_workgroup`
- **Database:** `default` (via Glue Catalog)
- **Tabelas disponÃ­veis:**
  - `refined_stocks` (dados refinados particionados)
  - `aggregated_stocks_monthly` (agregaÃ§Ãµes mensais)

**ConfiguraÃ§Ã£o:**
- Bucket de resultados: `<account_id>-athena-results-bucket`
- Output location: `s3://.../query_results/`
- Enforce workgroup configuration: true

**Exemplos de consultas possÃ­veis:**

```sql
-- Consultar dados refinados
SELECT * FROM refined_stocks 
WHERE nome_acao = 'itub4' 
AND data_pregao >= DATE('2024-07-01')
LIMIT 10;

-- Consultar agregados mensais
SELECT 
    nome_acao,
    mes_referencia,
    preco_medio_mensal,
    volume_total_mensal
FROM aggregated_stocks_monthly
ORDER BY mes_referencia DESC;

-- AnÃ¡lise de volatilidade por aÃ§Ã£o
SELECT 
    nome_acao,
    AVG(volatilidade_7d) as volatilidade_media
FROM refined_stocks
GROUP BY nome_acao;
```

---

## ğŸ—ï¸ Arquitetura Completa

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      PIPELINE BATCH B3                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. EXTRAÃ‡ÃƒO (DiÃ¡ria - 22:00 UTC)
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ EventBridge  â”‚ â†’ cron(0 22 * * ? *)
   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚   Lambda     â”‚ â†’ extract.py (yfinance)
   â”‚   Extract    â”‚
   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚      S3      â”‚ â†’ raw/execution_date=.../data_particao=.../
   â”‚   RAW        â”‚ â†’ Parquet particionado
   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“ (S3 Event: _SUCCESS)

2. TRANSFORMAÃ‡ÃƒO (Event-driven)
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚   Lambda     â”‚ â†’ trigger_glue.py
   â”‚   Trigger    â”‚
   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“ (glue.start_job_run)
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  Glue Job    â”‚ â†’ transform.py (Polars)
   â”‚  Transform   â”‚ â†’ Feature Engineering
   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚      S3      â”‚ â†’ refined/data_pregao=.../nome_acao=.../
   â”‚   REFINED    â”‚ â†’ agg/mes_referencia=.../
   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“ (boto3)

3. CATALOGAÃ‡ÃƒO (AutomÃ¡tica)
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Glue Catalog â”‚ â†’ Tables: refined_stocks, aggregated_stocks_monthly
   â”‚   + Crawler  â”‚ â†’ Database: default
   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“

4. CONSULTA (On-demand)
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚    Athena    â”‚ â†’ SQL queries
   â”‚   Workgroup  â”‚ â†’ etl_workgroup
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š Features Implementadas (AlÃ©m dos Requisitos)

### Robustez e Confiabilidade
1. **ProteÃ§Ã£o contra execuÃ§Ãµes concorrentes** (Glue job max_concurrent_runs = 1)
2. **Retry logic** no yfinance (mÃ©todo download + fallback para history)
3. **ValidaÃ§Ã£o de conectividade** antes de extrair
4. **Limpeza de dados nulos** antes de processar
5. **Error handling nÃ£o-bloqueante** na catalogaÃ§Ã£o

### Observabilidade
1. **Logs detalhados** em todas as etapas
2. **MÃ©tricas de execuÃ§Ã£o** (registros processados, aÃ§Ãµes Ãºnicas)
3. **Status tracking** via marker `_SUCCESS`
4. **CloudWatch integration** (Lambda + Glue)

### Qualidade dos Dados
1. **Arredondamento de floats** (2 casas decimais)
2. **Type casting explÃ­cito** (Date, String)
3. **OrdenaÃ§Ã£o por Ticker e Data**
4. **RemoÃ§Ã£o de registros com lags/mÃ©dias incompletos**

### Flexibilidade
1. **Suporte a formato wide e long** (conversÃ£o automÃ¡tica)
2. **DetecÃ§Ã£o de ambiente** (AWS Glue vs Local)
3. **Paths S3 ou locais** (desenvolvimento/produÃ§Ã£o)
4. **Argumentos configurÃ¡veis** (bucket, prefix)

---

## ğŸ”§ Melhorias Sugeridas

### Curto Prazo
1. âœ… **CatalogaÃ§Ã£o automÃ¡tica** - IMPLEMENTADO
2. **Adicionar mais tickers** (outras blue chips: VALE3, PETR4, etc.)
3. **Testes unitÃ¡rios** para transformaÃ§Ãµes
4. **DocumentaÃ§Ã£o de queries Athena** comuns

### MÃ©dio Prazo
1. **Alertas CloudWatch** (falhas de extraÃ§Ã£o/transformaÃ§Ã£o)
2. **Dashboard QuickSight** para visualizaÃ§Ã£o
3. **Backfill mechanism** para reprocessar dados histÃ³ricos
4. **Data quality checks** (Great Expectations ou similar)

### Longo Prazo
1. **OrquestraÃ§Ã£o via Step Functions** (melhor visibilidade)
2. **Versionamento de dados** (Delta Lake ou similar)
3. **Streaming pipeline** (dados em tempo real via Kinesis)
4. **Machine Learning** (previsÃ£o de preÃ§os, detecÃ§Ã£o de anomalias)

---

## âœ… Checklist Final

- [x] Requisito 1: ExtraÃ§Ã£o de dados B3 (yfinance)
- [x] Requisito 2: Dados brutos em S3 Parquet particionado
- [x] Requisito 3: S3 aciona Lambda que chama Glue
- [x] Requisito 4: Lambda em Python
- [x] Requisito 5A: Agrupamento e sumarizaÃ§Ã£o
- [x] Requisito 5B: RenomeaÃ§Ã£o de colunas
- [x] Requisito 5C: CÃ¡lculos temporais (mÃ©dias mÃ³veis, lags, volatilidade)
- [x] Requisito 6: Dados refined particionados
- [x] Requisito 7: CatalogaÃ§Ã£o automÃ¡tica no Glue Catalog
- [x] Requisito 8: Dados consultÃ¡veis via Athena

---

## ğŸ“ ConclusÃ£o

O projeto **atende completamente todos os 8 requisitos** do Tech Challenge apÃ³s a correÃ§Ã£o implementada para catalogaÃ§Ã£o automÃ¡tica.

A arquitetura Ã© robusta, bem estruturada e segue boas prÃ¡ticas de engenharia de dados:
- **Separation of concerns** (extraÃ§Ã£o, transformaÃ§Ã£o, catalogaÃ§Ã£o)
- **Event-driven architecture** (S3 notifications)
- **Infrastructure as Code** (Terraform)
- **Observabilidade** (logs detalhados, CloudWatch)
- **EficiÃªncia** (Polars para processamento, Parquet com compressÃ£o)

O cÃ³digo estÃ¡ pronto para produÃ§Ã£o e pode ser facilmente estendido para incluir mais tickers, features adicionais ou integraÃ§Ãµes com outros serviÃ§os AWS.
