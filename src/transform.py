"""
transform.py - Transforma√ß√£o de dados de a√ß√µes
Processa dados brutos (raw), aplica feature engineering e salva em:
- /refined: dados transformados particionados por data e nome da a√ß√£o
- /agg: dados agregados mensalmente
"""
import sys
import polars as pl
import polars.selectors as cs
from awsglue.utils import getResolvedOptions

print("=" * 80)
print("INICIANDO TRANSFORMA√á√ÉO DE DADOS - BLUE CHIPS B3")
print("=" * 80)

# L√™ argumentos passados pelo Glue Job
try:
    args = getResolvedOptions(sys.argv, ['JOB_NAME', 'BUCKET_NAME', 'INPUT_PREFIX'])
    bucket_name = args['BUCKET_NAME']
    input_prefix = args['INPUT_PREFIX']
except Exception:
    # Fallback: compatibilidade com INPUT_KEY (vers√µes antigas)
    args = getResolvedOptions(sys.argv, ['JOB_NAME', 'BUCKET_NAME', 'INPUT_KEY'])
    bucket_name = args['BUCKET_NAME']
    input_key = args['INPUT_KEY']
    input_prefix = input_key.rsplit('/', 1)[0] + '/'

input_path = f"s3://{bucket_name}/{input_prefix}"
print(f"\nüì• Lendo dados de: {input_path}\n")

# ============================================================================
# 1. LEITURA E LIMPEZA DOS DADOS RAW
# ============================================================================

# L√™ dados raw em formato Parquet com Polars
df_raw = pl.read_parquet(input_path)
print(f"‚úì Dados carregados: {df_raw.shape[0]:,} registros, {df_raw.shape[1]} colunas")
print(f"  Colunas: {', '.join(df_raw.columns)}\n")

# Normaliza tipos de dados e ordena
df_clean = df_raw.with_columns([
    pl.col("Ticker").cast(pl.Utf8, strict=False),
    pl.col("Date").cast(pl.Date, strict=False),
]).sort(["Ticker", "Date"])

# Remove registros com valores nulos nas colunas essenciais
df_clean = df_clean.filter(
    pl.col("Ticker").is_not_null() & 
    pl.col("Date").is_not_null() &
    pl.col("Close").is_not_null()
)

print(f"‚úì Ap√≥s limpeza: {df_clean.shape[0]:,} registros\n")

# ============================================================================
# 2. FEATURE ENGINEERING
# ============================================================================

print("üîß Aplicando transforma√ß√µes e criando features...\n")

df_refined = df_clean.with_columns([
    # Renomeia e padroniza colunas
    pl.col("Date").alias("data_pregao"),
    pl.col("Ticker").str.replace(".SA", "").str.to_lowercase().alias("nome_acao"),
    pl.col("Open").alias("abertura"),
    pl.col("Close").alias("fechamento"),
    pl.col("High").alias("max"),
    pl.col("Low").alias("min"),
    pl.col("Volume").alias("volume_negociado"),
    
    # M√©dias m√≥veis (7, 14 e 30 dias)
    pl.col("Close").rolling_mean(window_size=7).over("Ticker").alias("media_movel_7d"),
    pl.col("Close").rolling_mean(window_size=14).over("Ticker").alias("media_movel_14d"),
    pl.col("Close").rolling_mean(window_size=30).over("Ticker").alias("media_movel_30d"),
    
    # Lags (1, 2 e 3 dias anteriores)
    pl.col("Close").shift(1).over("Ticker").alias("lag_1d"),
    pl.col("Close").shift(2).over("Ticker").alias("lag_2d"),
    pl.col("Close").shift(3).over("Ticker").alias("lag_3d"),
    
    # Varia√ß√£o percentual di√°ria
    ((pl.col("Close") - pl.col("Open")) / pl.col("Open") * 100).alias("variacao_pct_dia"),
    
    # Amplitude do dia (diferen√ßa entre m√°xima e m√≠nima)
    (pl.col("High") - pl.col("Low")).alias("amplitude_dia"),
    
    # Volatilidade (desvio padr√£o m√≥vel de 7 dias)
    pl.col("Close").rolling_std(window_size=7).over("Ticker").alias("volatilidade_7d"),
])

# Remove registros com nulls (geralmente dos primeiros dias por causa de lags/m√©dias)
df_refined = df_refined.drop_nulls()

# Arredonda valores float para 2 casas decimais
df_refined = df_refined.with_columns(cs.float().round(2))

# Seleciona colunas finais na ordem desejada
df_final = df_refined.select([
    "data_pregao",
    "nome_acao",
    "abertura",
    "fechamento",
    "max",
    "min",
    "volume_negociado",
    "variacao_pct_dia",
    "amplitude_dia",
    "media_movel_7d",
    "media_movel_14d",
    "media_movel_30d",
    "volatilidade_7d",
    "lag_1d",
    "lag_2d",
    "lag_3d",
])

print(f"‚úì Features criadas: {df_final.shape[1]} colunas")
print(f"‚úì Registros finais: {df_final.shape[0]:,}\n")

# ============================================================================
# 3. SALVAR DADOS REFINED (PARTICIONADOS POR DATA E NOME DA A√á√ÉO)
# ============================================================================

output_path_refined = f"s3://{bucket_name}/refined/"
print(f"üíæ Salvando dados REFINED em: {output_path_refined}")
print(f"   Particionamento: data_pregao + nome_acao\n")

df_final.write_parquet(
    output_path_refined,
    use_pyarrow=True,
    partition_by=["data_pregao", "nome_acao"],
    compression="snappy"
)

print("‚úì Dados refined salvos com sucesso!\n")

# ============================================================================
# 4. DADOS AGREGADOS MENSAIS
# ============================================================================

print("üìä Gerando agrega√ß√µes mensais...\n")

df_agregado = df_refined.group_by([
    "nome_acao",
    pl.col("data_pregao").dt.truncate("1mo").alias("mes_referencia")
]).agg([
    # Agrega√ß√µes de pre√ßo
    pl.col("fechamento").mean().alias("preco_medio_mensal"),
    pl.col("fechamento").min().alias("preco_minimo_mensal"),
    pl.col("fechamento").max().alias("preco_maximo_mensal"),
    
    # Agrega√ß√µes de volume
    pl.col("volume_negociado").sum().alias("volume_total_mensal"),
    pl.col("volume_negociado").mean().alias("volume_medio_diario"),
    
    # Estat√≠sticas de varia√ß√£o
    pl.col("variacao_pct_dia").mean().alias("variacao_media_diaria_pct"),
    pl.col("volatilidade_7d").mean().alias("volatilidade_media_mensal"),
    
    # Contagem de dias de negocia√ß√£o
    pl.col("data_pregao").n_unique().alias("dias_negociacao"),
]).sort(["nome_acao", "mes_referencia"])

# Arredonda valores
df_agregado = df_agregado.with_columns(cs.float().round(2))

print(f"‚úì Agrega√ß√µes geradas: {df_agregado.shape[0]:,} registros mensais\n")

# Salva dados agregados
output_path_agg = f"s3://{bucket_name}/agg/"
print(f"üíæ Salvando dados AGREGADOS em: {output_path_agg}\n")

df_agregado.write_parquet(
    output_path_agg,
    use_pyarrow=True,
    compression="snappy"
)

print("‚úì Dados agregados salvos com sucesso!\n")

# ============================================================================
# RESUMO FINAL
# ============================================================================

print("=" * 80)
print("‚úÖ TRANSFORMA√á√ÉO CONCLU√çDA COM SUCESSO!")
print("=" * 80)
print(f"üìä Estat√≠sticas finais:")
print(f"   ‚Ä¢ Registros refined:  {df_final.shape[0]:,}")
print(f"   ‚Ä¢ Registros agregados: {df_agregado.shape[0]:,}")
print(f"   ‚Ä¢ A√ß√µes processadas:  {df_final['nome_acao'].n_unique()}")
print(f"   ‚Ä¢ Features criadas:   {df_final.shape[1]}")
print("=" * 80)